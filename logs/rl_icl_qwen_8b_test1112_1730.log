2025-11-12 17:30:26 - root - INFO - Logging setup complete. Outputting to console and file: logs/rl_icl_qwen_8b_test1112_1730.log
2025-11-12 17:30:26 - root - INFO - Initialized all random seeds to: 42
2025-11-12 17:30:26 - __main__ - INFO - Using device: cuda
2025-11-12 17:30:26 - __main__ - INFO - Starting run: 1112_1730
2025-11-12 17:30:26 - __main__ - INFO - --- Initializing Models ---
2025-11-12 17:30:26 - root - INFO - [EmbeddingModel] Loading embedding model: all-MiniLM-L6-v2...
2025-11-12 17:30:26 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-11-12 17:31:07 - root - INFO - [EmbeddingModel] Model loaded. Embedding dimension: 384
2025-11-12 17:31:07 - models.llm_wrapper - INFO - Loading LLM environment: Qwen/Qwen3-8B...
2025-11-12 17:31:26 - models.llm_wrapper - INFO - LLM 'Qwen/Qwen3-8B' loaded successfully.
2025-11-12 17:31:26 - data_utils.mtop_loader - INFO - --- Creating Corpus (Action Space) ---
2025-11-12 17:31:26 - data_utils.mtop_loader - INFO - Loading full 'train' split (no global cache used)...
2025-11-12 17:31:44 - data_utils.mtop_loader - INFO - Loading EmbeddingModel (all-MiniLM-L6-v2) for K-Means/Sampling...
2025-11-12 17:31:44 - root - INFO - [EmbeddingModel] Loading embedding model: all-MiniLM-L6-v2...
2025-11-12 17:31:44 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2025-11-12 17:31:58 - root - INFO - [EmbeddingModel] Model loaded. Embedding dimension: 384
2025-11-12 17:31:58 - data_utils.mtop_loader - INFO - Pre-computing all training data embeddings...
2025-11-12 17:32:00 - data_utils.mtop_loader - INFO - Full train data and embeddings loaded (15667 samples).
2025-11-12 17:32:00 - data_utils.mtop_loader - WARNING - USE_CLUSTERED_CORPUS=False. Using the *entire* training dataset (15667 samples) as corpus.
2025-11-12 17:32:00 - data_utils.mtop_loader - INFO - Corpus created. Final size: 15667
2025-11-12 17:32:00 - __main__ - INFO - Corpus embeddings computed. Shape: torch.Size([15667, 384])
2025-11-12 17:32:00 - data_utils.mtop_loader - INFO - Loading query dataloader (split=train, batch_size=16, shuffle=True, nums=5000)...
2025-11-12 17:32:20 - data_utils.mtop_loader - INFO - Randomly sampling 5000 examples for 'train' split...
2025-11-12 17:32:20 - data_utils.mtop_loader - INFO - Loading query dataloader (split=dev, batch_size=64, shuffle=False, nums=128)...
2025-11-12 17:32:26 - data_utils.mtop_loader - INFO - Randomly sampling 128 examples for 'dev' split...
2025-11-12 17:32:26 - root - INFO - sytem prompt is You are an expert assistant for semantic parsing. Given a user utterance, you must convert it into its logical form representation.
2025-11-12 17:32:39 - __main__ - INFO - Accuracy: 48.44% (62/128), Avg NLL: 0.0079
2025-11-12 17:32:39 - __main__ - INFO - Saving evaluation results to file...
2025-11-12 17:32:39 - __main__ - INFO - Evaluation results saved to: results/rl_icl_qwen_8b_test/1112_1730/val_results_test.csv
2025-11-12 17:32:39 - root - INFO - sytem prompt is You are an expert assistant for semantic parsing. Convert the user utterance into its logical form using the schema below
        Definition: IN:: Represents the user's Intent. SL:: Represents a Slot (a parameter for the intent).
        Allowed Schema: You must use only the intents and slots from these lists:
        Intents: [GET_RECIPES, GET_INFO_RECIPES]  Slots: [RECIPES_DISH, RECIPES_TYPE RECIPES_ATTRIBUTE, RECIPES_INCLUDED_INGREDIENT]
        
2025-11-12 17:32:55 - __main__ - INFO - Accuracy: 46.88% (60/128), Avg NLL: 0.0069
2025-11-12 17:32:55 - __main__ - INFO - Saving evaluation results to file...
2025-11-12 17:32:55 - __main__ - INFO - Evaluation results saved to: results/rl_icl_qwen_8b_test/1112_1730/val_results_test.csv
