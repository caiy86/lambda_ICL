{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f74d28e",
   "metadata": {},
   "source": [
    "我们分析了每个lambda下预测成功的embedding的分布情况，显示不同lambda下embedding的分布几乎一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b687f16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from config import train_config as config\n",
    "import utils\n",
    "import data_utils.mtop_loader as dataloader\n",
    "from models.embedding_model import EmbeddingModel\n",
    "from models.policy_network import RBFPolicyNetwork\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "MODEL_PATH =  \"cache/lambda_icl_qwen_0.6b/1210_2023_best.pt\"\n",
    "\n",
    "def check_model_distribution():\n",
    "\n",
    "    device = utils.device\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "\n",
    "    # 2. 初始化 Embedding Model (必须与 main.py 一致，用于编码 query)\n",
    "    logger.info(f\"Loading EmbeddingModel: {config.EMBEDDING_MODEL_NAME} ...\")\n",
    "    embedding_model = EmbeddingModel(model_name=config.EMBEDDING_MODEL_NAME)\n",
    "\n",
    "    # 3. 初始化 Policy Network (Agent)\n",
    "    logger.info(\"Initializing RBFPolicyNetwork (num_centers=1024)...\")\n",
    "    agent = RBFPolicyNetwork(\n",
    "        embedding_dim=embedding_model.dim,\n",
    "        num_centers=1024,  \n",
    "        dropout=0.0       \n",
    "    ).to(device)\n",
    "\n",
    "    # 4. 加载模型权重\n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        logger.info(f\"Loading checkpoint from: {MODEL_PATH}\")\n",
    "        try:\n",
    "            state_dict = torch.load(MODEL_PATH, map_location=device)\n",
    "\n",
    "            if isinstance(state_dict, dict) and 'model_state_dict' in state_dict:\n",
    "                agent.load_state_dict(state_dict['model_state_dict'])\n",
    "            else:\n",
    "                agent.load_state_dict(state_dict)\n",
    "            logger.info(\"Model loaded successfully.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load model: {e}\")\n",
    "            return\n",
    "    else:\n",
    "        logger.error(f\"Model file not found: {MODEL_PATH}\")\n",
    "        return\n",
    "\n",
    "    agent.eval()\n",
    "\n",
    "    # 5. 获取数据 (使用 dev 集)\n",
    "    # 模拟 main.py 中的 val_loader\n",
    "    logger.info(\"Loading Validation Data (split='dev')...\")\n",
    "    val_loader = dataloader.get_dataloader(\n",
    "        split='dev', \n",
    "        batch_size=64, \n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    # 6. 推理\n",
    "    all_actions = []\n",
    "    all_probs = []\n",
    "    \n",
    "    logger.info(\"Running Inference...\")\n",
    "    \n",
    "    # 只取第一个 batch 进行详细分析\n",
    "    batch_data = next(iter(val_loader))\n",
    "    \n",
    "    # main.py 的 dataloader 返回的是 List[Dict]\n",
    "    query_texts = [item['query'] for item in batch_data]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # 实时编码 (同 main.py)\n",
    "        query_embs = embedding_model.encode(query_texts) # Tensor on device\n",
    "        \n",
    "        # 获取模型输出\n",
    "        logits, values = agent.get_logits_and_values(query_embs)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        actions = torch.argmax(probs, dim=-1)\n",
    "        \n",
    "        all_actions = actions.cpu().numpy()\n",
    "        all_probs = probs.cpu().numpy()\n",
    "\n",
    "    # 7. 绘图与分析\n",
    "    plot_analysis(all_actions, all_probs)\n",
    "\n",
    "def plot_analysis(actions, probs):\n",
    "    \"\"\"\n",
    "    绘制详细的分布图\n",
    "    \"\"\"\n",
    "    num_samples = len(actions)\n",
    "    # 假设 Action 0-20 对应 Lambda 0.0 - 1.0 (step 0.05)\n",
    "    lambda_values = np.arange(21) * 0.05\n",
    "    \n",
    "    plt.figure(figsize=(15, 6))\n",
    "\n",
    "    # --- 图 1: 最终选择的 Lambda 计数 ---\n",
    "    plt.subplot(1, 2, 1)\n",
    "    # 统计每个 Lambda 被选中的次数\n",
    "    counts = np.bincount(actions, minlength=21)\n",
    "    \n",
    "    plt.bar(lambda_values, counts, width=0.04, color='teal', alpha=0.8, edgecolor='black')\n",
    "    plt.title(f'Selected Lambda Distribution (Batch Size={num_samples})')\n",
    "    plt.xlabel('Lambda Value')\n",
    "    plt.ylabel('Count (Frequency)')\n",
    "    plt.xticks(np.arange(0, 1.1, 0.1))\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "    # --- 图 2: 平均概率分布 (模型是否犹豫) ---\n",
    "    plt.subplot(1, 2, 2)\n",
    "    mean_probs = np.mean(probs, axis=0)\n",
    "    \n",
    "    plt.bar(lambda_values, mean_probs, width=0.04, color='skyblue', edgecolor='blue', alpha=0.8)\n",
    "    plt.title('Average Predicted Probability (Model Confidence)')\n",
    "    plt.xlabel('Lambda Value')\n",
    "    plt.ylabel('Avg Probability')\n",
    "    plt.xticks(np.arange(0, 1.1, 0.1))\n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "    # 计算平均熵\n",
    "    entropy = -np.sum(probs * np.log(probs + 1e-9), axis=1).mean()\n",
    "    plt.text(0.05, 0.9, f\"Avg Entropy: {entropy:.4f}\", transform=plt.gca().transAxes, \n",
    "             fontsize=12, bbox=dict(facecolor='white', alpha=0.8))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- 打印文字报告 ---\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(f\"Analysis Report (Samples: {num_samples})\")\n",
    "    print(\"=\"*40)\n",
    "    print(f\"Avg Entropy: {entropy:.4f} (Max possible: {np.log(21):.4f})\")\n",
    "    \n",
    "    print(\"\\nTop 5 Preferred Lambdas (by selection count):\")\n",
    "    top_indices = np.argsort(counts)[::-1][:5]\n",
    "    for idx in top_indices:\n",
    "        if counts[idx] > 0:\n",
    "            print(f\"  Lambda {idx*0.05:.2f}: {counts[idx]} samples\")\n",
    "            \n",
    "    print(\"\\nTop 5 Highest Probability Mass (on average):\")\n",
    "    top_prob_indices = np.argsort(mean_probs)[::-1][:5]\n",
    "    for idx in top_prob_indices:\n",
    "        print(f\"  Lambda {idx*0.05:.2f}: {mean_probs[idx]:.4f}\")\n",
    "    \n",
    "    # 检查是否是“平顶山”\n",
    "    std_probs = np.std(mean_probs)\n",
    "    if std_probs < 0.02:\n",
    "        print(\"\\n[WARNING] Probability distribution is very flat! The model fails to distinguish between actions.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    check_model_distribution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b414bc9",
   "metadata": {},
   "source": [
    "我们继续分析成功预测和失败预测 embedding的分布情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56c0e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Initializing models...\n",
      "INFO:root:[EmbeddingModel] Loading embedding model: all-MiniLM-L6-v2...\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "INFO:root:[EmbeddingModel] Model loaded. Embedding dimension: 384\n",
      "INFO:models.llm_wrapper:Loading LLM environment: Qwen/Qwen3-0.6B...\n",
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "INFO:models.llm_wrapper:LLM 'Qwen/Qwen3-0.6B' loaded successfully.\n",
      "INFO:models.policy_network:Initializing RBFPolicyNetwork: Input=384, Centers=1024 (RBF Layer)\n",
      "INFO:__main__:Loading checkpoint: cache/lambda_icl_qwen_0.6b/1210_2023_best.pt\n",
      "INFO:data_utils.mtop_loader:--- Creating Corpus (Action Space) ---\n",
      "INFO:data_utils.mtop_loader:Loading full 'train' split (no global cache used)...\n",
      "INFO:data_utils.mtop_loader:Loading EmbeddingModel (all-MiniLM-L6-v2) for K-Means/Sampling...\n",
      "INFO:root:[EmbeddingModel] Loading embedding model: all-MiniLM-L6-v2...\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "INFO:root:[EmbeddingModel] Model loaded. Embedding dimension: 384\n",
      "INFO:data_utils.mtop_loader:Pre-computing all training data embeddings...\n",
      "Batches: 100%|██████████| 980/980 [00:03<00:00, 279.83it/s]\n",
      "INFO:data_utils.mtop_loader:Full train data and embeddings loaded (15667 samples).\n",
      "INFO:data_utils.mtop_loader:Corpus created. Final size: 15667\n",
      "INFO:data_utils.mtop_loader:Loading query dataloader (split=train, batch_size=16, shuffle=True, nums=256, seed=None)...\n",
      "INFO:data_utils.mtop_loader:Randomly sampling 256 examples for 'train' split...\n",
      "INFO:engine.sampler:EpisodeSampler initialized for 8 steps (MMR).\n",
      "INFO:engine.reward_computer:RewardComputer initialized with GAE (gamma=0.99, lambda=0.95)\n",
      "INFO:__main__:Collecting 10 batches to analyze Advantage distribution...\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 6.35 GiB. GPU 0 has a total capacity of 47.38 GiB of which 4.65 GiB is free. Including non-PyTorch memory, this process has 42.71 GiB memory in use. Of the allocated memory 39.58 GiB is allocated by PyTorch, and 2.73 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 160\u001b[0m\n\u001b[1;32m    157\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 160\u001b[0m     \u001b[43mcheck_advantage_stats\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 97\u001b[0m, in \u001b[0;36mcheck_advantage_stats\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m buffer \u001b[38;5;241m=\u001b[39m sampler\u001b[38;5;241m.\u001b[39mcollect_episodes(\n\u001b[1;32m     89\u001b[0m     query_batch\u001b[38;5;241m=\u001b[39mbatch_data,\n\u001b[1;32m     90\u001b[0m     corpus\u001b[38;5;241m=\u001b[39mcorpus_data,\n\u001b[1;32m     91\u001b[0m     corpus_embeddings\u001b[38;5;241m=\u001b[39mcorpus_embeddings\n\u001b[1;32m     92\u001b[0m )\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# B. 计算 Advantage (Compute)\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# 这步很关键，它会调用 Critic 计算 Value，并计算 GAE Advantage\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# 此时得到的 buffer.advantages 是原始的 (Raw)，还没被 Normalize\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m buffer \u001b[38;5;241m=\u001b[39m \u001b[43mreward_computer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_rewards_and_advantages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm_wrapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm_wrapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck_correct_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_correct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# 收集数据 (只取第一个 step 的，如果是单步任务)\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# buffer.advantages shape: [Batch, Steps]\u001b[39;00m\n\u001b[1;32m    105\u001b[0m advs \u001b[38;5;241m=\u001b[39m buffer\u001b[38;5;241m.\u001b[39madvantages\u001b[38;5;241m.\u001b[39mflatten()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/python/RetICL/my_reticl/engine/reward_computer.py:79\u001b[0m, in \u001b[0;36mRewardComputer.compute_rewards_and_advantages\u001b[0;34m(self, buffer, llm_wrapper, check_correct_fn)\u001b[0m\n\u001b[1;32m     76\u001b[0m     prompts\u001b[38;5;241m.\u001b[39mappend(prompt_str)\n\u001b[1;32m     77\u001b[0m     targets\u001b[38;5;241m.\u001b[39mappend(query_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 79\u001b[0m target_losses \u001b[38;5;241m=\u001b[39m \u001b[43mllm_wrapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batch_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m generated_texts, _ \u001b[38;5;241m=\u001b[39m llm_wrapper\u001b[38;5;241m.\u001b[39mgenerate_for_evaluation(\n\u001b[1;32m     82\u001b[0m     prompts, \n\u001b[1;32m     83\u001b[0m     max_new_tokens\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mMAX_GEN_TOKENS\n\u001b[1;32m     84\u001b[0m )\n\u001b[1;32m     86\u001b[0m rewards \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/ICL/lib/python3.10/site-packages/torch/utils/_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/python/RetICL/my_reticl/models/llm_wrapper.py:85\u001b[0m, in \u001b[0;36mLLMWrapper.get_batch_loss\u001b[0;34m(self, prompts, targets)\u001b[0m\n\u001b[1;32m     81\u001b[0m     labels[i, :prompt_token_lengths[i]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[1;32m     83\u001b[0m labels[labels \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mpad_token_id] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[0;32m---> 85\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m     93\u001b[0m shift_logits \u001b[38;5;241m=\u001b[39m logits[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "File \u001b[0;32m~/miniconda3/envs/ICL/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ICL/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/ICL/lib/python3.10/site-packages/transformers/utils/generic.py:918\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    917\u001b[0m     return_dict \u001b[38;5;241m=\u001b[39m return_dict_passed\n\u001b[0;32m--> 918\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    920\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/miniconda3/envs/ICL/lib/python3.10/site-packages/transformers/models/qwen3/modeling_qwen3.py:498\u001b[0m, in \u001b[0;36mQwen3ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    496\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 498\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m CausalLMOutputWithPast(\n\u001b[1;32m    501\u001b[0m     loss\u001b[38;5;241m=\u001b[39mloss,\n\u001b[1;32m    502\u001b[0m     logits\u001b[38;5;241m=\u001b[39mlogits,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    505\u001b[0m     attentions\u001b[38;5;241m=\u001b[39moutputs\u001b[38;5;241m.\u001b[39mattentions,\n\u001b[1;32m    506\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/ICL/lib/python3.10/site-packages/transformers/loss/loss_utils.py:55\u001b[0m, in \u001b[0;36mForCausalLMLoss\u001b[0;34m(logits, labels, vocab_size, num_items_in_batch, ignore_index, shift_labels, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mForCausalLMLoss\u001b[39m(\n\u001b[1;32m     46\u001b[0m     logits,\n\u001b[1;32m     47\u001b[0m     labels,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     53\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;66;03m# Upcast to float if we need to compute the loss to avoid potential precision issues\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shift_labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;66;03m# Shift so that tokens < n predict n\u001b[39;00m\n\u001b[1;32m     59\u001b[0m         labels \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mpad(labels, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m), value\u001b[38;5;241m=\u001b[39mignore_index)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 6.35 GiB. GPU 0 has a total capacity of 47.38 GiB of which 4.65 GiB is free. Including non-PyTorch memory, this process has 42.71 GiB memory in use. Of the allocated memory 39.58 GiB is allocated by PyTorch, and 2.73 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "from config import train_config as config\n",
    "import utils\n",
    "import data_utils.mtop_loader as dataloader\n",
    "from models.embedding_model import EmbeddingModel\n",
    "from models.llm_wrapper import LLMWrapper\n",
    "from models.policy_network import RBFPolicyNetwork\n",
    "from engine.sampler import EpisodeSampler\n",
    "from engine.reward_computer import RewardComputer\n",
    "\n",
    "MODEL_PATH = \"cache/lambda_icl_qwen_0.6b/1210_2023_best.pt\" \n",
    "CHECK_BATCHES = 10  # 检查多少个 Batch\n",
    "BATCH_SIZE = 16     # 每个 Batch 的大小\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def check_advantage_stats():\n",
    "    device = utils.device\n",
    "    \n",
    "    logger.info(\"Initializing models...\")\n",
    "    emb_model = EmbeddingModel(model_name=config.EMBEDDING_MODEL_NAME)\n",
    "    llm_wrapper = LLMWrapper(model_name=config.LLM_MODEL_NAME)\n",
    "    \n",
    "    agent = RBFPolicyNetwork(\n",
    "        embedding_dim=emb_model.dim,\n",
    "        num_centers=1024, \n",
    "        dropout=0.0\n",
    "    ).to(device)\n",
    "    \n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        logger.info(f\"Loading checkpoint: {MODEL_PATH}\")\n",
    "        state_dict = torch.load(MODEL_PATH, map_location=device)\n",
    "        if isinstance(state_dict, dict) and 'model_state_dict' in state_dict:\n",
    "            agent.load_state_dict(state_dict['model_state_dict'])\n",
    "        else:\n",
    "            agent.load_state_dict(state_dict)\n",
    "    else:\n",
    "        logger.warning(f\"Checkpoint not found at {MODEL_PATH}, using random init!\")\n",
    "\n",
    "    agent.eval()\n",
    "    \n",
    "    corpus_data, corpus_embeddings = dataloader.get_corpus()\n",
    "    train_loader = dataloader.get_dataloader(\n",
    "        split='train', batch_size=BATCH_SIZE, shuffle=True, nums=256\n",
    "    )\n",
    "    \n",
    "    sampler = EpisodeSampler(\n",
    "        policy_network=agent, \n",
    "        embedding_model=emb_model, \n",
    "        num_examples=config.NUM_EXAMPLES\n",
    "    )\n",
    "    \n",
    "    reward_computer = RewardComputer(\n",
    "        gamma=config.REWARD_GAMMA, \n",
    "        lambda_=config.REWARD_LAMBDA, \n",
    "        system_prompt=config.SYSTEM_PROMPT\n",
    "    )\n",
    "    \n",
    "    all_advantages = []\n",
    "    all_rewards = []\n",
    "    all_values = []\n",
    "    \n",
    "    logger.info(f\"Collecting {CHECK_BATCHES} batches to analyze Advantage distribution...\")\n",
    "    \n",
    "    iterator = iter(train_loader)\n",
    "    for i in range(CHECK_BATCHES):\n",
    "        try:\n",
    "            batch_data = next(iterator)\n",
    "        except StopIteration:\n",
    "            break\n",
    "            \n",
    "       \n",
    "        buffer = sampler.collect_episodes(\n",
    "            query_batch=batch_data,\n",
    "            corpus=corpus_data,\n",
    "            corpus_embeddings=corpus_embeddings\n",
    "        )\n",
    "        \n",
    "        # B. 计算 Advantage (Compute)\n",
    "        # 这步很关键，它会调用 Critic 计算 Value，并计算 GAE Advantage\n",
    "        # 此时得到的 buffer.advantages 是原始的 (Raw)，还没被 Normalize\n",
    "        buffer = reward_computer.compute_rewards_and_advantages(\n",
    "            buffer=buffer,\n",
    "            llm_wrapper=llm_wrapper,\n",
    "            check_correct_fn=dataloader.check_correct,\n",
    "        )\n",
    "        \n",
    "        # 收集数据 (只取第一个 step 的，如果是单步任务)\n",
    "        # buffer.advantages shape: [Batch, Steps]\n",
    "        advs = buffer.advantages.flatten().cpu().numpy()\n",
    "        rews = buffer.rewards.flatten().cpu().numpy()\n",
    "        vals = buffer.values.flatten().cpu().detach().numpy()\n",
    "        \n",
    "        all_advantages.extend(advs)\n",
    "        all_rewards.extend(rews)\n",
    "        all_values.extend(vals)\n",
    "        \n",
    "        print(f\"Batch {i+1}/{CHECK_BATCHES} processed.\")\n",
    "\n",
    "    # 4. 统计与可视化\n",
    "    all_advantages = np.array(all_advantages)\n",
    "    all_rewards = np.array(all_rewards)\n",
    "    all_values = np.array(all_values)\n",
    "    \n",
    "    # 阈值：绝对值小于 1e-4 视为 0\n",
    "    zero_mask = np.abs(all_advantages) < 1e-4\n",
    "    zero_ratio = np.sum(zero_mask) / len(all_advantages)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"ADVANTAGE DISTRIBUTION ANALYSIS\")\n",
    "    print(\"=\"*40)\n",
    "    print(f\"Total Samples: {len(all_advantages)}\")\n",
    "    print(f\"Zero Advantage Ratio (Abs < 1e-4): {zero_ratio:.2%}  <-- 重点关注这个\")\n",
    "    print(f\"Mean Advantage: {np.mean(all_advantages):.4f}\")\n",
    "    print(f\"Std Advantage:  {np.std(all_advantages):.4f}\")\n",
    "    print(\"-\" * 20)\n",
    "    print(\"Reward vs Value (Critic Accuracy):\")\n",
    "    print(f\"Mean Reward: {np.mean(all_rewards):.4f}\")\n",
    "    print(f\"Mean Value:  {np.mean(all_values):.4f}\")\n",
    "    \n",
    "    # 简单分析 Critic 是否已经学会了预测“死数据”\n",
    "    # 如果 Reward=1 且 Value≈1，或者 Reward=0 且 Value≈0，Advantage 就会是 0\n",
    "    accurate_prediction_mask = np.abs(all_rewards - all_values) < 0.1\n",
    "    print(f\"Accurate Critic Prediction Ratio (|R-V| < 0.1): {np.sum(accurate_prediction_mask)/len(all_values):.2%}\")\n",
    "\n",
    "    # 绘制直方图\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(all_advantages, bins=50, kde=False, color='purple')\n",
    "    plt.title('Raw Advantage Distribution')\n",
    "    plt.xlabel('Advantage value')\n",
    "    plt.ylabel('Count')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(all_values, all_rewards, alpha=0.3, s=10)\n",
    "    plt.title('Critic Value vs. Real Reward')\n",
    "    plt.xlabel('Predicted Value (Critic)')\n",
    "    plt.ylabel('Real Reward')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    check_advantage_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971d82bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "from config import train_config as config\n",
    "import utils\n",
    "import data_utils.mtop_loader as dataloader\n",
    "from models.embedding_model import EmbeddingModel\n",
    "from models.llm_wrapper import LLMWrapper\n",
    "from utils import device\n",
    "\n",
    "NUM_SAMPLES_TO_INSPECT =128\n",
    "OUTPUT_DIR = \"results/analysis\"\n",
    "OUTPUT_FILE = os.path.join(OUTPUT_DIR, \"inspection_report.csv\")\n",
    "\n",
    "LAMBDA_CANDIDATES = [0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "\n",
    "utils.setup_logging(log_level=\"INFO\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"Starting CSV Inspection. Output: {OUTPUT_FILE}\")\n",
    "\n",
    "print(\"Loading Models...\")\n",
    "embedding_model = EmbeddingModel(config.EMBEDDING_MODEL_NAME)\n",
    "llm_wrapper = LLMWrapper(config.LLM_MODEL_NAME)\n",
    "\n",
    "corpus_data, corpus_embeddings_cpu = dataloader.get_corpus()\n",
    "corpus_embeddings = corpus_embeddings_cpu.to(device)\n",
    "\n",
    "train_raw, _ = dataloader.get_train_val_split_data(\n",
    "    split='train',\n",
    "    train_nums=config.PRETRAIN_NUMS,\n",
    "    val_nums=64,\n",
    "    seed=config.PRETRAIN_SEED\n",
    ")\n",
    "\n",
    "inspect_data = train_raw[:NUM_SAMPLES_TO_INSPECT] if NUM_SAMPLES_TO_INSPECT else train_raw\n",
    "print(f\"Inspecting {len(inspect_data)} samples...\")\n",
    "\n",
    "rows = []\n",
    "\n",
    "for idx, sample in enumerate(tqdm(inspect_data, desc=\"Inspecting\")):\n",
    "    query_text = sample['query']\n",
    "    target_answer = sample['answer']\n",
    "    \n",
    "    row_data = {\n",
    "        \"Index\": idx,\n",
    "        \"Question\": query_text,\n",
    "        \"Answer\": target_answer,\n",
    "    }\n",
    "    \n",
    "    query_emb = embedding_model.encode([query_text]) # (1, D)\n",
    "    query_index = sample.get('corpus_index', -1)\n",
    "    \n",
    "    sim_scores_base = torch.matmul(query_emb, corpus_embeddings.T) # (1, Corpus)\n",
    "    \n",
    "    correct_lambdas = []\n",
    "\n",
    "    for lam_val in LAMBDA_CANDIDATES:\n",
    "        lambda_tensor = torch.tensor(lam_val, device=device)\n",
    "\n",
    "        selected_indices = []\n",
    "        selected_embs = torch.zeros((1, 0, embedding_model.dim), device=device)\n",
    "        \n",
    "        curr_mask = torch.zeros_like(sim_scores_base, dtype=torch.bool)\n",
    "        if query_index >= 0:\n",
    "            curr_mask[0, query_index] = True\n",
    "        \n",
    "        for t in range(config.NUM_EXAMPLES):\n",
    "            if t == 0:\n",
    "                step_scores = sim_scores_base.clone()\n",
    "            else:\n",
    "                sim_to_selected = torch.matmul(selected_embs, corpus_embeddings.T)\n",
    "                diversity_penalty, _ = torch.max(sim_to_selected, dim=1)\n",
    "                step_scores = (lambda_tensor * sim_scores_base) - ((1 - lambda_tensor) * diversity_penalty)\n",
    "            \n",
    "            step_scores.masked_fill_(curr_mask, -float('inf'))\n",
    "            best_idx = torch.argmax(step_scores, dim=1).item()\n",
    "            selected_indices.append(best_idx)\n",
    "            curr_mask[0, best_idx] = True\n",
    "            \n",
    "            new_emb = corpus_embeddings[best_idx].unsqueeze(0).unsqueeze(0)\n",
    "            selected_embs = torch.cat([selected_embs, new_emb], dim=1)\n",
    "\n",
    "        examples = [corpus_data[i] for i in selected_indices]\n",
    "\n",
    "        prompt = llm_wrapper.build_chat_prompt(config.SYSTEM_PROMPT, examples, query_text)\n",
    "        pred_texts, _ = llm_wrapper.generate_for_evaluation([prompt], max_new_tokens=config.MAX_GEN_TOKENS)\n",
    "        pred_text = pred_texts[0]\n",
    "\n",
    "        is_correct = dataloader.check_correct(target_answer, pred_text)\n",
    "        if is_correct:\n",
    "            correct_lambdas.append(lam_val)\n",
    "\n",
    "        mark = \"✅\" if is_correct else \"❌\"\n",
    "        row_data[f\"Pred_{lam_val}\"] = f\"{mark} {pred_text}\"\n",
    "        row_data[f\"Retrieved_{lam_val}\"] = ' '.join([str(ex) for ex in examples])\n",
    "\n",
    "    row_data[\"Is_Solvable\"] = len(correct_lambdas) > 0\n",
    "    row_data[\"Best_Lambdas\"] = str(correct_lambdas)\n",
    "    \n",
    "    rows.append(row_data)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "base_cols = [\"Index\", \"Is_Solvable\", \"Best_Lambdas\", \"Question\", \"Answer\"]\n",
    "\n",
    "lambda_cols = []\n",
    "for lam in LAMBDA_CANDIDATES:\n",
    "    lambda_cols.append(f\"Pred_{lam}\")\n",
    "    lambda_cols.append(f\"Retrieved_{lam}\")\n",
    "\n",
    "df = df[base_cols + lambda_cols]\n",
    "\n",
    "df = df.sort_values(by=[\"Is_Solvable\", \"Index\"], ascending=[False, True])\n",
    "\n",
    "df.to_csv(OUTPUT_FILE, index=False, encoding='utf-8-sig') # utf-8-sig 兼容 Excel 打开中文\n",
    "print(f\"Successfully saved inspection report to {OUTPUT_FILE}\")\n",
    "print(f\"Solvable Samples: {df['Is_Solvable'].sum()}/{len(df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea7404f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=\"sk-49da8b1a6c6d4f3e8e7a74860f2d11f1\", \n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an expert assistant for semantic parsing. Given a user utterance, you must convert it into its logical form representation.\"},\n",
    "    {\"role\": \"user\", \"content\": \"stop and delete the timer\"}, {\"role\": \"assistant\", \"content\": \"[IN:PAUSE_TIMER [SL:METHOD_TIMER timer ] ]\"},\n",
    "    {\"role\": \"user\", \"content\": \"close out of timer\"}, {\"role\": \"assistant\", \"content\": \"[IN:PAUSE_TIMER  [SL:METHOD_TIMER timer ] ]\"},\n",
    "    {\"role\": \"user\", \"content\": \"timer re-start\"}, {\"role\": \"assistant\", \"content\": \"[IN:RESUME_TIMER [SL:METHOD_TIMER timer ] ]\"},\n",
    "    {\"role\": \"user\", \"content\": \"alter timer\"}, {\"role\": \"assistant\", \"content\": \"[IN:UPDATE_TIMER [SL:METHOD_TIMER  timer ] ]\"},\n",
    "    {\"role\": \"user\", \"content\": \"Cancel timer\"}, {\"role\": \"assistant\", \"content\": \"[IN:DELETE_TIMER [SL:METHOD_TIMER timer ] ]\"},\n",
    "    # {\"role\": \"user\", \"content\": \"timer stop\"}, {\"role\": \"assistant\", \"content\": \"[IN:PAUSE_TIMER [SL:METHOD_TIMER timer ]  ]\"},\n",
    "    # {\"role\": \"user\", \"content\": \"stop and remove current timer\"}, {\"role\": \"assistant\", \"content\": \"[IN:PAUSE_TIMER [SL:METHOD_TIMER timer ] ]\"},\n",
    "    # {\"role\": \"user\", \"content\": \"turn off timer\"}, {\"role\": \"assistant\", \"content\": \"[IN:PAUSE_TIMER  [SL:METHOD_TIMER timer ] ]\"},\n",
    "    {\"role\": \"user\", \"content\": \"discontinue timer\"}\n",
    "]\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"qwen3-0.6b\",\n",
    "    messages=messages,\n",
    "    stream=True,\n",
    "    temperature=0,     \n",
    "    top_p=1.0,        \n",
    "    max_tokens=200,    \n",
    "    extra_body={\n",
    "        \"enable_thinking\": False,\n",
    "    }\n",
    ")\n",
    "\n",
    "reasoning_content = \"\" \n",
    "answer_content = \"\" \n",
    "is_answering = False  \n",
    "print(\"=\" * 20 + \"Thinking Process\" + \"=\" * 20 )\n",
    "\n",
    "for chunk in completion:\n",
    "    if not chunk.choices:\n",
    "        print(\"Usage:\")\n",
    "        print(chunk.usage)\n",
    "        continue\n",
    "\n",
    "    delta = chunk.choices[0].delta\n",
    "\n",
    "    if hasattr(delta, \"reasoning_content\") and delta.reasoning_content is not None:\n",
    "        if not is_answering:\n",
    "            print(delta.reasoning_content, end=\"\", flush=True)\n",
    "        reasoning_content += delta.reasoning_content\n",
    "\n",
    "    # Received content, starting to respond\n",
    "    if hasattr(delta, \"content\") and delta.content:\n",
    "        if not is_answering:\n",
    "            print(\"\\n\" + \"=\" * 20 + \"Complete Response\" + \"=\" * 20)\n",
    "            is_answering = True\n",
    "        print(delta.content, end=\"\", flush=True)\n",
    "        answer_content += delta.content\n",
    "\n",
    "    if is_answering and hasattr(chunk.choices[0], \"finish_reason\") and chunk.choices[0].finish_reason is not None:\n",
    "        print(\"\\n\" + \"=\" * 20 + \"Prompt Messages\" + \"=\" * 20)\n",
    "        # 将历史按 assistant:xxx\\n user:xxx\\n 格式输出\n",
    "        for msg in messages:\n",
    "            if msg[\"role\"] == \"assistant\":\n",
    "                print(f\"assistant: {msg['content']}\")\n",
    "            elif msg[\"role\"] == \"user\":\n",
    "                print(f\"user: {msg['content']}\")\n",
    "        print(f\"prediction: {answer_content.strip()}\")\n",
    "        print(\"=\" * 55)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ICL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
